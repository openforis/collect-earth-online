# usage `python3 data_migration.py [optional "loop"] [optional project to resume (must include [loop] or [file] for arg 1)]`
# example `python3 data_migration.py loop`
# example `python2 data_migration.py file 350`
import datetime
import json
import psycopg2
import shutil
import os
import glob
import csv
import re
import subprocess
import sys
from config import config

params = config()

# gets the right path depending upon whether developer uses gradle or maven
# compile dir for maven is target and for gradle it's build
isdir = os.path.isdir
jsonpath = r"../../../../target/classes/json"
if not isdir(jsonpath):
    jsonpath = r"../../../../build/resources/main/json"

csvpath = r"../../../../target/classes/csv"
if not isdir(csvpath):
    csvpath = r"../../../../build/resources/main/csv"
csvScriptPath = r"../csv"

shppath = r"../../../../target/classes/shp"
if not isdir(shppath):
    shppath = r"../../../../build/resources/main/shp"
shpScriptPath = r"../shp"

def truncate_all_tables():
    conn = None
    try:
        conn = psycopg2.connect(**params)
        cur = conn.cursor()
        cur.execute("TRUNCATE TABLE users RESTART IDENTITY CASCADE")
        cur.execute("TRUNCATE TABLE institutions RESTART IDENTITY CASCADE")
        cur.execute("TRUNCATE TABLE institution_users RESTART IDENTITY CASCADE")
        cur.execute("TRUNCATE TABLE imagery RESTART IDENTITY CASCADE")
        cur.execute("TRUNCATE TABLE projects RESTART IDENTITY CASCADE")
        cur.execute("TRUNCATE TABLE project_widgets RESTART IDENTITY CASCADE")
        cur.execute("TRUNCATE TABLE plots RESTART IDENTITY CASCADE")
        cur.execute("TRUNCATE TABLE samples RESTART IDENTITY CASCADE")
        cur.execute("TRUNCATE TABLE sample_values RESTART IDENTITY CASCADE")
        cur.execute("TRUNCATE TABLE roles RESTART IDENTITY CASCADE")
        conn.commit()
        cur.close()
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
            conn.close()

def insert_users():
    conn = None
    try:
        conn = psycopg2.connect(**params)
        cur = conn.cursor()
        dirname = os.path.dirname(os.path.realpath(__file__))
        user_list_json = open(os.path.abspath(os.path.realpath(os.path.join(dirname, jsonpath , "user-list.json"))), "r")
        users = json.load(user_list_json)
        print(len(users))
        for user in users:
            try:
                cur.execute("select * from add_user_migration(%s, %s::text, %s::text)", (user["id"], user["email"], user["password"]))
            except: pass
            conn.commit()
        cur.execute("select * from add_user_migration(%s, %s::text, %s::text)", (-1, "guest", "dkh*&jlkjadfjk&^58342bmdjkjhf(*&0984"))
        cur.execute("SELECT * FROM set_admin()")
        conn.commit()
        cur.close()
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
            conn.close()

def insert_institutions():
    conn = None
    try:
        conn = psycopg2.connect(**params)
        cur = conn.cursor()
        cur1 = conn.cursor()
        dirname = os.path.dirname(os.path.realpath(__file__))
        institution_list_json = open(os.path.abspath(os.path.realpath(os.path.join(dirname, jsonpath , "institution-list.json"))), "r")
        institutions = json.load(institution_list_json)
        print(len(institutions))
        for institution in institutions:
            members=institution["members"]
            admins=institution["admins"]
            pendingUsers=institution["pending"]
            cur.execute("select * from add_institution_migration(%s, %s::text, %s::text, %s::text, %s::text, %s)",
                        (institution["id"], institution["name"], institution["logo"], institution["description"], institution["url"], institution["archived"]))
            conn.commit()
            role_id = -1
            user_id = -1
            for member in members:
                if member in admins:
                    role_id = 1
                elif member in pendingUsers:
                    role_id = 3
                else:
                    role_id = 2
                if isinstance(member , int):
                    try:
                        cur1.execute("select * from add_institution_user(%s, %s, %s)", (institution["id"], member, role_id))
                    except: pass
                    conn.commit()
            for pending in pendingUsers:
                if pending not in members and pending not in admins:
                    try:
                        cur1.execute("select * from add_institution_user(%s, %s, %s)", (institution["id"], pending, 3))
                    except: pass
                    conn.commit()
        cur.close()
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
            conn.close()

def insert_imagery():
    conn = None
    try:
        conn = psycopg2.connect(**params)
        cur = conn.cursor()
        dirname = os.path.dirname(os.path.realpath(__file__))
        imagery_list_json = open(os.path.abspath(os.path.realpath(os.path.join(dirname, jsonpath , "imagery-list.json"))), "r")
        imageryArr = json.load(imagery_list_json)
        print(len(imageryArr))
        for imagery in imageryArr:
            if imagery["institution"] > 0:
                try:
                    cur.execute("select * from add_institution_imagery_migration(%s, %s, %s::text, %s::text, %s::text, %s::jsonb, %s::jsonb)",
                        (imagery["id"], imagery["institution"], imagery["visibility"], imagery["title"],
                        imagery["attribution"], json.dumps(imagery["extent"]), json.dumps(imagery["sourceConfig"])))
                except: pass
                conn.commit()
        cur.close()
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
            conn.close()

def insert_project_widgets(project_id, dash_id, conn):
    try:
        cur = conn.cursor()
        dirname = os.path.dirname(os.path.realpath(__file__))
        dash_json = open(os.path.abspath(os.path.realpath(os.path.join(dirname, jsonpath , "dash-" + dash_id + ".json"))), "r")
        widget = json.load(dash_json)
        if widget["projectID"] is not None and int(project_id)==int(widget["projectID"]) and len(str(widget["widgets"]))>2:
            for awidget in widget["widgets"]:
                cur.execute("select * from add_project_widget(%s, %s::uuid, %s::jsonb)",
                    (widget["projectID"], widget["dashboardID"], json.dumps(awidget)))
                conn.commit()
        cur.close()
    except (Exception, psycopg2.DatabaseError) as error:
        print("project widgets: "+ str(error))

def insert_projects():
    conn = None
    try:
        conn = psycopg2.connect(**params)
        cur = conn.cursor()
        dirname = os.path.dirname(os.path.realpath(__file__))
        project_list_json = open(os.path.abspath(os.path.realpath(os.path.join(dirname, jsonpath , "project-list.json"))), "r")
        projectArr = json.load(project_list_json)
        project_dash_list_json = open(os.path.abspath(os.path.realpath(os.path.join(dirname, jsonpath , "proj.json"))), "r")
        dashArr = json.load(project_dash_list_json)
        print(len(projectArr))
        for project in projectArr:
            try:
                if project["id"] > (int(sys.argv[2]) if len(sys.argv) > 2 else 0):
                    print("inserting project with project id " + str(project["id"]))
                    if project["numPlots"] is None: project["numPlots"]=0
                    if project["plotSpacing"] is None: project["plotSpacing"]=0
                    if project["plotSize"] is None: project["plotSize"]=0
                    if project["samplesPerPlot"] is None: project["samplesPerPlot"]=0
                    if project["sampleResolution"] is None: project["sampleResolution"]=0
                    if not ("projectTemplate" in project): project["projectTemplate"]=0
                    if not ("surveyRules" in project): project["surveyRules"]=[]
                    if not ("created_date" in project): project["created_date"]=None
                    if not ("published_date" in project): project["published_date"]=None
                    if not ("closed_date" in project): project["closed_date"]=None
                    if not ("archived_date" in project): project["archived_date"]=None
                    if not ("projectOptions" in project): project["projectOptions"]=None

                    cur.execute("select * from create_project_migration(%s, %s, %s::text, %s::text, %s::text, %s::text, "
                    + "ST_SetSRID(ST_GeomFromGeoJSON(%s), 4326), %s, %s::text, %s, %s, %s::text, %s, %s::text, "
                    + "%s, %s, %s::jsonb, %s::jsonb, %s::jsonb, %s::date, %s::date, %s::date, %s::date, %s::jsonb)",
                    (project["id"], project["institution"], project["availability"],
                    project["name"], project["description"], project["privacyLevel"], project["boundary"].replace("\\", ""),
                    project["imageryId"], project["plotDistribution"], project["numPlots"],
                    project["plotSpacing"], project["plotShape"], project["plotSize"], project["sampleDistribution"],
                    project["samplesPerPlot"], project["sampleResolution"], json.dumps(project["sampleValues"]),
                    json.dumps(project["surveyRules"]),None, project["created_date"], project["published_date"],
                    project["closed_date"], project["archived_date"], json.dumps(project["projectOptions"])))

                    project_id = project["id"]
                    for dash in dashArr:
                        dash_id = dash["dashboard"]
                        try:
                            if int(dash["projectID"]) == int(project_id):
                                insert_project_widgets(project_id,dash_id,conn)
                        except: pass

                    if len(sys.argv) > 1 and sys.argv[1] == "loop":
                        ## insert data by row with loops
                        insert_plots(project_id, conn)
                    else:
                        ## insert data the entire json file at a time
                        conn.commit()
                        insert_plots_samples_by_file(project_id, conn)

                    conn.commit()
                    # merge in external files
                    merge_files(project, project_id, conn)
            except(Exception, psycopg2.DatabaseError) as error:
                print("project for loop: "+ str(error))
                conn.commit()
        cur.close()
    except (Exception, psycopg2.DatabaseError) as error:
        print("project outer: "+ str(error))
    finally:
        if conn is not None:
            conn.close()

def insert_plots_samples_by_file(project_id, mainconn):
    conn = psycopg2.connect(**params)
    cur_plot = conn.cursor()
    try:
        print("insert by file")
        dirname = os.path.dirname(os.path.realpath(__file__))
        filename = os.path.abspath(os.path.realpath(os.path.join(dirname, jsonpath , "plot-data-" + str(project_id) + ".json")))
        if os.path.isfile(filename):
            filedata = open(filename, "r").read()
            cur_plot.execute("select * from add_plots_by_json(%s, %s::text)", (project_id, filedata))
            conn.commit()
    except (Exception, psycopg2.DatabaseError) as error:
        print("plot file error: "+ str(error))
        conn.commit()
        cur_plot.close()
        conn.close()
        insert_plots(project_id, mainconn)
    cur_plot.close()
    conn.close()

def insert_plots(project_id, conn):
    print("inserting plot the old way")
    cur_plot = conn.cursor()
    user_plot_id=-1
    dirname = os.path.dirname(os.path.realpath(__file__))
    filename = os.path.abspath(os.path.realpath(os.path.join(dirname, jsonpath , "plot-data-" + str(project_id) + ".json")))
    if os.path.isfile(filename):
        plot_list_json = open(filename, "r")
        plotArr = json.load(plot_list_json)
        for plot in plotArr:
            try:
                boolean_Flagged=plot["flagged"]
                if plot["flagged"] == False:
                    plot["flagged"] = 0
                else:
                    plot["flagged"] = 1

                if not ("collectionStart" in plot): plot["collectionStart"] = None
                if not ("collectionTime" in plot) or re.search("^\d+$", plot["collectionTime"]) is None : plot["collectionTime"]=None

                cur_plot.execute("select * from create_project_plot(%s, ST_SetSRID(ST_GeomFromGeoJSON(%s), 4326))",
                (project_id, plot["center"]))

                plot_id = cur_plot.fetchone()[0]
                if plot["user"] is not None:
                    user_plot_id = insert_user_plots(plot_id, plot, boolean_Flagged, conn)
                insert_samples(plot_id, plot["samples"], user_plot_id, conn)
                conn.commit()
            except (Exception, psycopg2.DatabaseError) as error:
                print("plots error: "+ str(error))
    cur_plot.close()

def insert_user_plots(plot_id, plot, flagged, conn):
    user_plot_id = -1
    cur_up = conn.cursor()
    cur_user = conn.cursor()
    cur_user.execute("select user_uid from users where email=%s;", [plot["user"]])
    rows = cur_user.fetchall()
    if len(rows)>0:
        try:
            cur_up.execute("select * from add_user_plots_migration(%s, %s::text, %s, to_timestamp(%s/1000.0)::timestamp, to_timestamp(%s/1000.0)::timestamp)",
                (plot_id, plot["user"], flagged, plot["collectionStart"], plot["collectionTime"]))
            user_plot_id = cur_up.fetchone()[0]
            conn.commit()
            user_plot_id = 1
        except (Exception, psycopg2.DatabaseError) as error:
                print("user plots error: "+ str(error))
    cur_up.close()
    cur_user.close()
    return user_plot_id

def insert_samples(plot_id, samples, user_plot_id, conn):
    cur_sample = conn.cursor()
    for sample in samples:
        try:
            cur_sample.execute("select * from create_project_plot_sample(%s, ST_SetSRID(ST_GeomFromGeoJSON(%s), 4326))",
             (plot_id, sample["point"] ))

            sample_id = cur_sample.fetchone()[0]
            if user_plot_id != -1 and "value" in sample:
                if not ("userImage" in sample):
                    sample["image_id"] = None
                    sample["image_attributes"] = None
                else:
                    sample["image_id"] = sample["userImage"]["id"]
                    sample["image_attributes"] = sample["userImage"]["attributes"]
                insert_sample_values(user_plot_id, sample_id, sample["value"], sample["image_id"], sample["image_attributes"], conn)
            conn.commit()
        except (Exception, psycopg2.DatabaseError) as error:
                print("samples error: "+ str(error))
    cur_sample.close()

def insert_sample_values(user_plot_id, sample_id, sample_value, image_id, image_value, conn):
    cur_sv = conn.cursor()
    try:
        cur_sv.execute("select * from add_sample_values_migration(%s, %s, %s::jsonb, %s, %s::jsonb)",
            (user_plot_id, sample_id, json.dumps(sample_value), image_id, json.dumps(image_value)))
    except (Exception, psycopg2.DatabaseError) as error:
                print("sample values error: "+ str(error))
    conn.commit()
    cur_sv.close()

# builds list of old name to become lon, lat
def csvColRenameList(csvCols):
    renameCol = []
    renameCol.append([csvCols[0].replace(" ", ""), "lon"])
    renameCol.append([csvCols[1].replace(" ", ""), "lat"])
    return renameCol

# files should have lat lon in the first 2 columns, returns string
def csvHeaderToCol(csvCols):
    colsStr = ""
    for i, h in enumerate(csvCols):
        h = h.replace(" ", "")
        if len(h) > 0:
            if i <=1:
                colsStr += h + " float,"
            elif (i == 2 and h.upper() == "PLOTID") or (i == 3 and h.upper() == "SAMPLEID"):
                colsStr += h + " integer,"
            else:
                colsStr += h + " text,"
    return colsStr[:-1]

def checkRequiredCols(actualCols, reqCols):
    for col in reqCols:
        if not (col.upper() in (acol.upper() for acol in actualCols)):
            return []
    return actualCols

def isFloat(string):
    try:
        float(string)
        return True
    except ValueError:
        return False

# returns list
def loadCsvHeaders(csvfile):
    with open(csvfile, "r") as fin:
        csvin = csv.reader(fin)
        headers = next(csvin, [])
        firstRow = next(csvin, [])
        if headers[0].upper() == "PLOTID" or headers[0].upper() == "ID" or (not isFloat(firstRow[0]) or -180.0 > float(firstRow[0]) > 180.0 or
           not isFloat(firstRow[1]) or -90.0 > float(firstRow[1]) > 90.0):
                print("Error with columns in file " + csvfile)
                return []
        else:
            return headers

def merge_files(project, project_id, conn):
    try:
        print("merging external files")
        cur = conn.cursor()
        plots_table = ""
        samples_table = ""
        dirname = os.path.dirname(os.path.realpath(__file__))
        need_to_update = 0
        fileprefix = "project-" +  str(project_id)
        tableprefix = "project_" +  str(project_id)

        # old files do not have a plotId, all columns are extra
        oldFilename = os.path.abspath(os.path.realpath(os.path.join(dirname, csvpath , fileprefix + ".csv")))
        filename = os.path.abspath(os.path.realpath(os.path.join(dirname, csvpath , fileprefix + "-plots.csv")))

        if project["plotDistribution"] == "csv" and (os.path.isfile(oldFilename)):
            csv_headers = loadCsvHeaders(oldFilename)
            if len(csv_headers) > 0:
                plots_table = tableprefix + "_plots_csv"
                need_to_update = 1
                cur.execute("DROP TABLE IF EXISTS ext_tables." + plots_table)
                cur.execute("SELECT * FROM create_new_table(%s, %s)",
                [plots_table, csvHeaderToCol(csv_headers)])
                conn.commit()


                # run sh to upload csv to postgres
                dirname = os.path.dirname(os.path.realpath(__file__))
                shpath = os.path.abspath(os.path.realpath(os.path.join(dirname, csvpath)))
                subprocess.run(["bash", "csv2postgres-alt.sh", fileprefix, fileprefix + "-plots"], cwd=shpath, stdout=subprocess.PIPE)

                # add index
                cur.execute("SELECT * FROM add_index_col(%s)" , [plots_table])
                conn.commit()

                #rename cols
                try:
                    colList = csvColRenameList(csv_headers)
                    if colList[0][0].upper() != colList[0][1].upper():
                        cur.execute("SELECT * FROM rename_col(%s, %s, %s)" , [plots_table, colList[0][0], colList[0][1]])
                        conn.commit()

                    if colList[1][0].upper() != colList[1][1].upper():
                        cur.execute("SELECT * FROM rename_col(%s, %s, %s)" , [plots_table, colList[1][0], colList[1][1]])
                        conn.commit()
                except:
                    pass

                # if column plotid does exist, it is not the same as the newer plotId field
                try:
                    cur.execute("SELECT * FROM rename_col(%s, %s, %s)" , [plots_table, "plotId", "plotId2"])
                    conn.commit()
                except:
                    conn.commit()
                    pass
                # no requirements for plotId mean that even if the field exists its not valid.  Add empty column for
                # backwords compatibility
                cur.execute("SELECT * FROM add_plotId_col(%s)" , [plots_table])
                conn.commit()

        elif project["plotDistribution"] == "csv" and (os.path.isfile(filename)):
            csv_headers = loadCsvHeaders(filename)
            if len(checkRequiredCols(csv_headers, ["plotId"])) > 0:
                plots_table = tableprefix + "_plots_csv"
                need_to_update = 2
                cur.execute("DROP TABLE IF EXISTS ext_tables." + plots_table)
                cur.execute("SELECT * FROM create_new_table(%s, %s)",
                [plots_table, csvHeaderToCol(csv_headers)])
                conn.commit()

                # run sh to upload csv to postgres
                dirname = os.path.dirname(os.path.realpath(__file__))
                shpath = os.path.abspath(os.path.realpath(os.path.join(dirname, csvpath)))
                subprocess.run(["bash", "csv2postgres.sh", fileprefix + "-plots"], cwd=shpath, stdout=subprocess.PIPE)

                # add index
                cur.execute("SELECT * FROM add_index_col(%s)" , [plots_table])
                conn.commit()

                #rename cols
                try:
                    colList = csvColRenameList(csv_headers)
                    if colList[0][0].upper() != colList[0][1].upper():
                        cur.execute("SELECT * FROM rename_col(%s, %s, %s)" , [plots_table, colList[0][0], colList[0][1]])
                        conn.commit()

                    if colList[1][0].upper() != colList[1][1].upper():
                        cur.execute("SELECT * FROM rename_col(%s, %s, %s)" , [plots_table, colList[1][0], colList[1][1]])
                        conn.commit()
                except:
                    pass

            else:
                print (csv_headers)

        if project["plotDistribution"] == "shp":
            filename = os.path.abspath(os.path.realpath(os.path.join(dirname, shppath , fileprefix + "-plots.zip")))
            templateFilename = os.path.abspath(os.path.realpath(os.path.join(dirname, shppath , "project-" + str(project["projectTemplate"]) + "-plots.zip")))
            if (not os.path.isfile(filename)) and os.path.isfile(templateFilename):
                shutil.copy(templateFilename, filename)

            if os.path.isfile(filename):
                need_to_update = 2
                plots_table = "project_" +  str(project_id) + "_plots_shp"
                cur.execute("DROP TABLE IF EXISTS ext_tables." + plots_table)
                conn.commit()
                # run sh
                dirname = os.path.dirname(os.path.realpath(__file__))
                shpath = os.path.abspath(os.path.realpath(os.path.join(dirname, shppath)))
                subprocess.run(["bash", "shp2postgres.sh", fileprefix + "-plots"], cwd=shpath, stdout=subprocess.PIPE)

        # ### Samples
        filename = os.path.abspath(os.path.realpath(os.path.join(dirname, csvpath , fileprefix + "-samples.csv")))
        if project["sampleDistribution"] == "csv" and os.path.isfile(filename):
            csv_headers = loadCsvHeaders(filename)
            if len(checkRequiredCols(csv_headers, ["plotId", "sampleId"])) > 0:
                samples_table = "project_" +  str(project_id) + "_samples_csv"
                need_to_update = 3
                cur.execute("DROP TABLE IF EXISTS ext_tables." + samples_table)
                cur.execute("SELECT * FROM create_new_table(%s, %s)",
                [samples_table, csvHeaderToCol(csv_headers)])
                conn.commit()

                # run sh to upload csv to postgres
                dirname = os.path.dirname(os.path.realpath(__file__))
                shpath = os.path.abspath(os.path.realpath(os.path.join(dirname, csvpath)))
                subprocess.run(["bash", "csv2postgres.sh", fileprefix + "-samples"], cwd=shpath, stdout=subprocess.PIPE)

                # add index
                cur.execute("SELECT * FROM add_index_col(%s)" , [samples_table])

                #rename cols
                try:
                    colList = csvColRenameList(csv_headers)
                    if colList[0][0].upper() != colList[0][1].upper():
                        cur.execute("SELECT * FROM rename_col(%s, %s, %s)" , [samples_table, colList[0][0], colList[0][1]])
                        conn.commit()

                    if colList[1][0].upper() != colList[1][1].upper():
                        cur.execute("SELECT * FROM rename_col(%s, %s, %s)" , [samples_table, colList[1][0], colList[1][1]])
                        conn.commit()
                except:
                    pass

        if project["sampleDistribution"] == "shp":
            filename = os.path.abspath(os.path.realpath(os.path.join(dirname, shppath , fileprefix + "-samples.zip")))
            templateFilename = os.path.abspath(os.path.realpath(os.path.join(dirname, shppath , "project-" + str(project["projectTemplate"]) + "-samples.zip")))
            if (not os.path.isfile(filename)) and os.path.isfile(templateFilename):
                shutil.copy(templateFilename, filename)

            if os.path.isfile(filename):
                need_to_update = 3

                samples_table = "project_" +  str(project_id) + "_samples_shp"
                cur.execute("DROP TABLE IF EXISTS ext_tables." + samples_table)
                conn.commit()
                # run sh
                dirname = os.path.dirname(os.path.realpath(__file__))
                shpath = os.path.abspath(os.path.realpath(os.path.join(dirname, shppath)))
                subprocess.run(["bash", "shp2postgres.sh", fileprefix + "-samples"], cwd=shpath, stdout=subprocess.PIPE)

        if need_to_update > 0:
            try:
                # add table names to project
                cur.execute("SELECT * FROM update_project_tables(%s, %s, %s)" , [project_id, plots_table, samples_table])
                conn.commit()

                if need_to_update >= 2:
                    # clean up project tables
                    cur.execute("SELECT * FROM cleanup_project_tables(%s, %s)" , [project_id, project["plotSize"]])
                    conn.commit()

                if need_to_update == 3:
                    # merge files into plots and samples
                    cur.execute("SELECT * FROM merge_plot_and_file(%s)" , [project_id])
                    conn.commit()
                else:
                    # merge files into plots
                    cur.execute("SELECT * FROM merge_plots_only(%s)" , [project_id])
                    conn.commit()
            finally:
                pass

    except (Exception, psycopg2.DatabaseError) as error:
        print("merge file error: "+ str(error))
        conn.commit()
        cur.close()

def insert_roles():
    conn = None
    try:
        conn = psycopg2.connect(**params)
        cur = conn.cursor()
        cur.execute("INSERT INTO roles VALUES (%s, %s::text)", (1, "admin"))
        cur.execute("INSERT INTO roles VALUES (%s, %s::text)", (2, "member"))
        cur.execute("INSERT INTO roles VALUES (%s, %s::text)", (3, "pending"))
        conn.commit()
        cur.close()
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
            conn.close()
def update_sequence():
    conn = psycopg2.connect(**params)
    cur = conn.cursor()
    cur.execute("select * from update_sequence('users', 'user_uid')")
    cur.execute("select * from update_sequence('institutions', 'institution_uid')")
    cur.execute("select * from update_sequence('imagery', 'imagery_uid')")
    cur.execute("select * from update_sequence('projects', 'project_uid')")
    cur.execute("select * from update_sequence('user_plots', 'user_plot_uid')")
    cur.execute("select * from update_sequence('sample_values', 'sample_value_uid')")
    cur.execute("select * from update_sequence('plots', 'plot_uid')")

if __name__ == "__main__":
    print(datetime.datetime.now().strftime("%Y-%m-%d %H:%M"))
    csvScripts = os.listdir(csvScriptPath)
    for f in csvScripts:
        if f.endswith(".sh"):
            shutil.copy(csvScriptPath+"/"+f, csvpath)

    shpScripts = os.listdir(shpScriptPath)
    for f in shpScripts:
        if f.endswith(".sh"):
            shutil.copy(shpScriptPath+"/"+f, shppath)

    if (len(sys.argv) <= 2):
        truncate_all_tables()
        print("inserting users")
        insert_users()
        print("inserting roles")
        insert_roles()
        print("inserting institutions")
        insert_institutions()
        print("inserting imagery")
        insert_imagery()

    print("inserting projects")
    insert_projects()
    print("Done with projects")
    update_sequence()
    print("Done migration")
    print(datetime.datetime.now().strftime("%Y-%m-%d %H:%M"))
